import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime
import time
import re
import gspread
from google.oauth2.service_account import Credentials
import numpy as np
import os

def is_regular_stock(name):
    """
    ì¼ë°˜ ì£¼ì‹ì¸ì§€ íŒë‹¨í•˜ëŠ” í•¨ìˆ˜
    ETF, í€ë“œ, ì±„ê¶Œ ë“±ì„ ì œì™¸í•˜ê³  ìˆœìˆ˜ ì£¼ì‹ ì¢…ëª©ë§Œ ì„ ë³„
    """
    
    # ëª…ë°±í•œ ETF/í€ë“œ ë¸Œëœë“œëª…ë“¤
    etf_brands = [
        'KODEX', 'TIGER', 'KBSTAR', 'KOSEF', 'KINDEX', 'ARIRANG', 'HANARO',
        'RISE', 'ACE', 'KIWOOM', 'PLUS', 'SOL', 'WON', '1Q', 'ITF', 'BNK',
        'FOCUS', 'TREX', 'HK', 'íŒŒì›Œ', 'ë§ˆì´í‹°', 'DAISHIN343', 'ì•„ì´ì— ì—ì…‹',
        'KCGI', 'KBë°œí•´ì¸í”„ë¼', 'ë§¥ì¿¼ë¦¬ì¸í”„ë¼', 'ë§µìŠ¤ë¦¬ì–¼í‹°', 'í•œêµ­ANKORìœ ì „'
    ]
    
    # ETF/í€ë“œ ê´€ë ¨ í‚¤ì›Œë“œ
    fund_keywords = [
        'ETF', 'ETN', 'REIT', 'ë¦¬ì¸ ', 'í€ë“œ',
        'ì±„ê¶Œ', 'í†µì•ˆì±„', 'ë¬¼ê°€ì±„', 'ê¸ˆìœµì±„', 'êµ­ê³ ì±„', 'íšŒì‚¬ì±„',
        'ë‹¨ê¸°ìê¸ˆ', 'ë‹¨ê¸°í†µì•ˆì±„', 'ë‹¨ê¸°ê¸ˆìœµì±„', 'ì•¡í‹°ë¸Œ',
        'ìŠ¤íŒ©', 'SPAC'  # SPAC(íŠ¹ìˆ˜ëª©ì ì¸ìˆ˜íšŒì‚¬) ì¶”ê°€
    ]
    
    # íˆ¬ì ê´€ë ¨ í‚¤ì›Œë“œ (ì¼ë°˜ ì£¼ì‹ì—ëŠ” ì—†ëŠ”)
    investment_keywords = [
        'ì¸ë±ìŠ¤', 'í•©ì„±', 'ì„ ë¬¼', 'ì˜µì…˜', 'ì¸ë²„ìŠ¤', 'ë ˆë²„ë¦¬ì§€',
        'ì»¤ë²„ë“œì½œ', 'ìœ„í´ë¦¬ì»¤ë²„ë“œì½œ', 'ë°ì¼ë¦¬ì»¤ë²„ë“œì½œ', 'ê³ ì •ì»¤ë²„ë“œì½œ',
        'TOP', 'Plus', 'TR', 'í¬ì»¤ìŠ¤', 'í…Œë§ˆ', 'ë°¸ë¥˜', 'ì„±ì¥', 'ì†Œë¶€ì¥'
    ]
    
    # ì§€ìˆ˜ ê´€ë ¨ í‚¤ì›Œë“œ
    index_keywords = ['S&P', 'MSCI', 'CSI', 'FTSE', 'Nikkei', 'DAX', 'NASDAQ', 'SOLACTIVE', 'KRX']
    
    # êµ­ê°€/ì§€ì—­ ê´€ë ¨ (í•´ì™¸íˆ¬ì ETF)
    country_keywords = ['ë¯¸êµ­', 'ì¤‘êµ­', 'ì¼ë³¸', 'ë…ì¼', 'ì¸ë„', 'ê¸€ë¡œë²Œ', 'ì•„ì‹œì•„', 'ìœ ë¡œ']
    
    # 1. ETF ë¸Œëœë“œëª… ì²´í¬
    for brand in etf_brands:
        if name.startswith(brand):
            return False
    
    # 2. í€ë“œ/ì±„ê¶Œ í‚¤ì›Œë“œ ì²´í¬
    for keyword in fund_keywords:
        if keyword in name:
            return False
    
    # 3. íˆ¬ì ê´€ë ¨ í‚¤ì›Œë“œ ì²´í¬
    for keyword in investment_keywords:
        if keyword in name:
            return False
    
    # 4. ì§€ìˆ˜ ê´€ë ¨ í‚¤ì›Œë“œ ì²´í¬
    for keyword in index_keywords:
        if keyword in name:
            return False
    
    # 5. êµ­ê°€/ì§€ì—­ í‚¤ì›Œë“œ ì²´í¬ (ë‹¨, ì‹¤ì œ ê¸°ì—…ëª…ì— í¬í•¨ëœ ê²½ìš°ëŠ” ì˜ˆì™¸)
    for keyword in country_keywords:
        if keyword in name:
            # ì‹¤ì œ ê¸°ì—…ëª…ì˜ ì¼ë¶€ê°€ ì•„ë‹ˆë¼ íˆ¬ì ìƒí’ˆëª…ì¸ ê²½ìš°ë§Œ ì œì™¸
            if any(invest_word in name for invest_word in ['ë°°ë‹¹', 'ì„±ì¥', 'í…Œí¬', 'ë°˜ë„ì²´', 'ë‚˜ìŠ¤ë‹¥']):
                return False
    
    # 6. ì¦ê¶Œì‚¬ ìš´ìš© ìƒí’ˆ (ì‹¤ì œ ì¦ê¶ŒíšŒì‚¬ ì£¼ì‹ì€ ì œì™¸í•˜ì§€ ì•ŠìŒ)
    securities_products = [
        'NHíˆ¬ì', 'SKì¦ê¶Œ', 'ë©”ë¦¬ì¸ ', 'ë¯¸ë˜ì—ì…‹', 'ì‚¼ì„±ì„ ë¬¼', 'ì‹ í•œíˆ¬ì',
        'ìœ ì•ˆíƒ€', 'ìœ ì§„íˆ¬ì', 'í‚¤ì›€', 'í•˜ë‚˜ê¸ˆìœµ', 'í•œí™”íˆ¬ì',
        'KBì¦ê¶Œ', 'IBKíˆ¬ì', 'êµë³´ì¦ê¶Œ', 'ëŒ€ì‹ ì¦ê¶Œ', 'í˜„ëŒ€ì°¨ì¦ê¶Œ'
    ]
    
    for keyword in securities_products:
        if keyword in name and not ('ì¦ê¶Œ' in name or 'ê¸ˆìœµ' in name or 'ì€í–‰' in name):
            return False
    
    # 7. ê³ ë°°ë‹¹, ë°°ë‹¹ ê´€ë ¨ ìƒí’ˆ (ê°œë³„ ì£¼ì‹ì´ ì•„ë‹Œ í…Œë§ˆ ìƒí’ˆ)
    if 'ê³ ë°°ë‹¹' in name or ('ë°°ë‹¹' in name and any(word in name for word in ['TOP', 'Plus', 'ì„±ì¥', 'í‚¹'])):
        return False
    
    # 8. SPAC (íŠ¹ìˆ˜ëª©ì ì¸ìˆ˜íšŒì‚¬) ê´€ë ¨ íŒ¨í„´
    spac_patterns = [
        'ìŠ¤íŒ©', 'SPAC', 'ëª©ì ', 'ì¸ìˆ˜íšŒì‚¬', 'íŠ¹ìˆ˜ëª©ì '
    ]
    
    for pattern in spac_patterns:
        if pattern in name:
            return False
    
    # SPAC ëª…ëª… íŒ¨í„´: "íšŒì‚¬ëª…ìŠ¤íŒ©ìˆ«ìí˜¸" (ì˜ˆ: ì—”ì—ì´ì¹˜ìŠ¤íŒ©29í˜¸)
    if 'ìŠ¤íŒ©' in name and ('í˜¸' in name or any(char.isdigit() for char in name)):
        return False
    
    # 9. ê¸°íƒ€ íˆ¬ìíšŒì‚¬ íŒ¨í„´
    if 'íˆ¬ìíšŒì‚¬' in name or 'ìì‚°ìš´ìš©' in name:
        return False
    
    return True

class GoogleSheetsUploader:
    def __init__(self, credentials_file='credentials.json'):
        """êµ¬ê¸€ ì‹œíŠ¸ ì—…ë¡œë” ì´ˆê¸°í™”"""
        self.credentials_file = credentials_file
        self.gc = None
        self.setup_connection()
    
    def setup_connection(self):
        """êµ¬ê¸€ ì‹œíŠ¸ ì—°ê²° ì„¤ì •"""
        try:
            scope = [
                'https://www.googleapis.com/auth/spreadsheets',
                'https://www.googleapis.com/auth/drive'
            ]
            
            if not os.path.exists(self.credentials_file):
                print(f"âŒ {self.credentials_file} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                return False
            
            creds = Credentials.from_service_account_file(self.credentials_file, scopes=scope)
            self.gc = gspread.authorize(creds)
            print("âœ… êµ¬ê¸€ ì‹œíŠ¸ ì—°ê²° ì„±ê³µ!")
            return True
            
        except Exception as e:
            print(f"âŒ êµ¬ê¸€ ì‹œíŠ¸ ì—°ê²° ì‹¤íŒ¨: {str(e)}")
            return False
    
    def create_or_get_spreadsheet(self, spreadsheet_name):
        """ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ìƒì„± ë˜ëŠ” ê°€ì ¸ì˜¤ê¸°"""
        try:
            # ê¸°ì¡´ ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ì°¾ê¸°
            try:
                spreadsheet = self.gc.open(spreadsheet_name)
                print(f"ğŸ“‹ ê¸°ì¡´ ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ì‚¬ìš©: {spreadsheet_name}")
                return spreadsheet
            except gspread.SpreadsheetNotFound:
                # ìƒˆ ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ìƒì„±
                spreadsheet = self.gc.create(spreadsheet_name)
                print(f"ğŸ“ ìƒˆ ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ìƒì„±: {spreadsheet_name}")
                return spreadsheet
                
        except Exception as e:
            print(f"âŒ ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ìƒì„±/ì ‘ê·¼ ì‹¤íŒ¨: {str(e)}")
            return None
    
    def clean_dataframe(self, df):
        """DataFrameì—ì„œ NaN ê°’ì„ ì²˜ë¦¬í•˜ì—¬ JSON í˜¸í™˜ ê°€ëŠ¥í•˜ê²Œ ë§Œë“¤ê¸°"""
        # DataFrame ë³µì‚¬
        df_clean = df.copy()
        
        # NaN ê°’ì„ ë¹ˆ ë¬¸ìì—´ë¡œ ë³€ê²½
        df_clean = df_clean.fillna('')
        
        # inf, -inf ê°’ë„ ì²˜ë¦¬
        df_clean = df_clean.replace([np.inf, -np.inf], '')
        
        # ëª¨ë“  ê°’ì„ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ JSON í˜¸í™˜ì„± ë³´ì¥
        for col in df_clean.columns:
            df_clean[col] = df_clean[col].astype(str)
            # 'nan' ë¬¸ìì—´ë„ ë¹ˆ ë¬¸ìì—´ë¡œ ë³€ê²½
            df_clean[col] = df_clean[col].replace('nan', '')
        
        return df_clean
    
    def upload_dataframe(self, df, spreadsheet_name, sheet_name):
        """ë°ì´í„°í”„ë ˆì„ì„ êµ¬ê¸€ ì‹œíŠ¸ì— ì—…ë¡œë“œ (NaN ê°’ ì²˜ë¦¬ í¬í•¨)"""
        try:
            spreadsheet = self.create_or_get_spreadsheet(spreadsheet_name)
            if not spreadsheet:
                return False
            
            # ì‹œíŠ¸ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ì—†ìœ¼ë©´ ìƒì„±
            try:
                worksheet = spreadsheet.worksheet(sheet_name)
                # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
                worksheet.clear()
            except gspread.WorksheetNotFound:
                # ìƒˆ ì‹œíŠ¸ ìƒì„±
                worksheet = spreadsheet.add_worksheet(title=sheet_name, rows=2000, cols=30)
            
            # ë°ì´í„° ì—…ë¡œë“œ
            if len(df) > 0:
                # DataFrame ì •ë¦¬ (NaN ê°’ ì²˜ë¦¬)
                df_clean = self.clean_dataframe(df)
                
                # í—¤ë”ì™€ ë°ì´í„°ë¥¼ í•¨ê»˜ ì—…ë¡œë“œ
                data = [df_clean.columns.tolist()] + df_clean.values.tolist()
                
                # ìˆ˜ì •ëœ update ë°©ì‹
                worksheet.update(values=data, range_name='A1')
                
                print(f"âœ… '{sheet_name}' ì‹œíŠ¸ì— {len(df)}ê°œ í–‰ ì—…ë¡œë“œ ì™„ë£Œ!")
                return True
            else:
                print(f"âš ï¸ '{sheet_name}' ì‹œíŠ¸: ì—…ë¡œë“œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return True
                
        except Exception as e:
            print(f"âŒ ì‹œíŠ¸ ì—…ë¡œë“œ ì‹¤íŒ¨ ({sheet_name}): {str(e)}")
            return False
    
    def get_spreadsheet_url(self, spreadsheet_name):
        """ìŠ¤í”„ë ˆë“œì‹œíŠ¸ URL ê°€ì ¸ì˜¤ê¸°"""
        try:
            spreadsheet = self.gc.open(spreadsheet_name)
            return spreadsheet.url
        except:
            return None

    def format_sheet_headers(self, spreadsheet_name, sheet_name):
        """ì‹œíŠ¸ í—¤ë” í¬ë§· ì„¤ì • (ì„ íƒì‚¬í•­)"""
        try:
            spreadsheet = self.gc.open(spreadsheet_name)
            worksheet = spreadsheet.worksheet(sheet_name)
            
            # í—¤ë” í–‰ êµµê²Œ ë§Œë“¤ê¸°
            worksheet.format('A1:Z1', {
                'textFormat': {'bold': True},
                'backgroundColor': {'red': 0.9, 'green': 0.9, 'blue': 0.9}
            })
            
            print(f"âœ… '{sheet_name}' ì‹œíŠ¸ í—¤ë” í¬ë§· ì ìš© ì™„ë£Œ!")
            return True
            
        except Exception as e:
            print(f"âš ï¸ í—¤ë” í¬ë§· ì ìš© ì‹¤íŒ¨ ({sheet_name}): {str(e)}")
            return False

def calculate_volume_change_rate(current_volume, prev_volume):
    """ê±°ë˜ëŸ‰ ì „ì¼ë¹„ ì¦ê°ìœ¨ ê³„ì‚°"""
    try:
        if not current_volume or not prev_volume or prev_volume == '0':
            return ''
        
        current = float(str(current_volume).replace(',', ''))
        previous = float(str(prev_volume).replace(',', ''))
        
        if previous == 0:
            return ''
        
        change_rate = ((current - previous) / previous) * 100
        return f"{change_rate:.2f}"
    except:
        return ''

def get_individual_stock_data(code, name):
    """ê°œë³„ ì¢…ëª© í˜ì´ì§€ì—ì„œ ìƒì„¸ ì¬ë¬´ ë°ì´í„° ìˆ˜ì§‘"""
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'}
    
    try:
        url = f'https://finance.naver.com/item/main.naver?code={code}'
        response = requests.get(url, headers=headers)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ì´ˆê¸°í™” (18ê°œ ë°ì´í„° í•„ë“œ)
        data = {
            'PER': '', 'PBR': '', 'ROE': '', 'ì‹œê°€ì´ì•¡': '',
            'ë§¤ì¶œì•¡': '', 'ì˜ì—…ì´ìµ': '', 'ë‹¹ê¸°ìˆœì´ìµ': '', 
            'ë¶€ì±„ë¹„ìœ¨': '', 'ìœ ë³´ìœ¨': '', 'ë°°ë‹¹ìˆ˜ìµë¥ ': '', 'ë°°ë‹¹ê¸ˆ': '',
            '52ì£¼ìµœê³ ': '', '52ì£¼ìµœì €': '', 'ê±°ë˜ëŒ€ê¸ˆ': '',
            'ì™¸êµ­ì¸ë¹„ìœ¨': '', 'ê¸°ê´€ë¹„ìœ¨': '', 'ë² íƒ€': '', 'ì—…ì¢…': ''
        }
        
        # ì—…ì¢… ì •ë³´ ì¶”ì¶œ (ì—¬ëŸ¬ ë°©ë²• ì‹œë„)
        # ë°©ë²• 1: ì¢…ëª©ëª… ì˜† ë§í¬ì—ì„œ
        sector_link = soup.select_one('.wrap_company h2 a')
        if sector_link and sector_link.get('title'):
            data['ì—…ì¢…'] = sector_link.get('title').strip()
        
        # ë°©ë²• 2: ê¸°ì—…ê°œìš” í˜ì´ì§€ì—ì„œ
        if not data['ì—…ì¢…']:
            try:
                company_url = f'https://finance.naver.com/item/coinfo.naver?code={code}'
                company_response = requests.get(company_url, headers=headers)
                company_soup = BeautifulSoup(company_response.text, 'html.parser')
                
                # ì—…ì¢… í…Œì´ë¸”ì—ì„œ ì¶”ì¶œ
                tables = company_soup.find_all('table')
                for table in tables:
                    rows = table.find_all('tr')
                    for row in rows:
                        cells = row.find_all(['td', 'th'])
                        for i, cell in enumerate(cells[:-1]):
                            if 'ì—…ì¢…' in cell.get_text(strip=True) and i+1 < len(cells):
                                data['ì—…ì¢…'] = cells[i+1].get_text(strip=True)
                                break
                        if data['ì—…ì¢…']:
                            break
                    if data['ì—…ì¢…']:
                        break
            except:
                pass
        
        # ê±°ë˜ëŒ€ê¸ˆê³¼ ì‹œê°€ì´ì•¡ ì¶”ì¶œ (ë©”ì¸ í˜ì´ì§€ì˜ ìš”ì•½ ì •ë³´ì—ì„œ)
        today_data = soup.select('.today .blind')
        for elem in today_data:
            text = elem.get_text(strip=True)
            # ê±°ë˜ëŒ€ê¸ˆ ì¶”ì¶œ
            if 'ê±°ë˜ëŒ€ê¸ˆ' in text:
                match = re.search(r'ê±°ë˜ëŒ€ê¸ˆ[^\d]*([,\d]+)', text)
                if match:
                    data['ê±°ë˜ëŒ€ê¸ˆ'] = match.group(1).replace(',', '')
        
        # ì‹œê°€ì´ì•¡ì€ ë³´í†µ ìš”ì•½ ì •ë³´ í…Œì´ë¸”ì— ìˆìŒ
        summary_table = soup.select_one('table.no_info')
        if summary_table:
            for row in summary_table.find_all('tr'):
                cells = row.find_all(['td', 'th'])
                for i, cell in enumerate(cells[:-1]):
                    cell_text = cell.get_text(strip=True)
                    if 'ì‹œê°€ì´ì•¡' in cell_text and i+1 < len(cells):
                        next_text = cells[i+1].get_text(strip=True)
                        # ì‹œê°€ì´ì•¡ì—ì„œ ìˆ«ìë§Œ ì¶”ì¶œ (ì–µì› ë‹¨ìœ„)
                        match = re.search(r'([,\d]+)ì–µ?ì›?', next_text)
                        if match:
                            data['ì‹œê°€ì´ì•¡'] = match.group(1).replace(',', '')
        
        # ë² íƒ€ ì •ë³´ëŠ” íˆ¬ìì •ë³´ ì„¹ì…˜ì—ì„œ ì¶”ì¶œ
        invest_info = soup.select('.section.invest_info')
        for section in invest_info:
            text = section.get_text()
            beta_match = re.search(r'ë² íƒ€[^\d]*([+-]?\d+\.?\d*)', text)
            if beta_match:
                data['ë² íƒ€'] = beta_match.group(1)
                break
        
        # ì¶”ê°€: ì „ì²´ í˜ì´ì§€ í…ìŠ¤íŠ¸ì—ì„œ ì •ê·œì‹ìœ¼ë¡œ ëˆ„ë½ëœ ë°ì´í„° ë³´ì™„
        page_text = soup.get_text()
        
        # ë” í¬ê´„ì ì¸ íŒ¨í„´ìœ¼ë¡œ ì¬ì‹œë„
        if not data['ê±°ë˜ëŒ€ê¸ˆ']:
            trading_match = re.search(r'ê±°ë˜ëŒ€ê¸ˆ[^\d]*([,\d]+)ë°±ë§Œì›?|ê±°ë˜ëŒ€ê¸ˆ[^\d]*([,\d]+)ì–µì›?', page_text)
            if trading_match:
                value = trading_match.group(1) or trading_match.group(2)
                data['ê±°ë˜ëŒ€ê¸ˆ'] = value.replace(',', '') if value else ''
        
        if not data['ì‹œê°€ì´ì•¡']:
            market_cap_match = re.search(r'ì‹œê°€ì´ì•¡[^\d]*([,\d]+)ì–µì›?', page_text)
            if market_cap_match:
                data['ì‹œê°€ì´ì•¡'] = market_cap_match.group(1).replace(',', '')
        
        if not data['ë² íƒ€']:
            beta_match = re.search(r'ë² íƒ€[^\d]*([+-]?\d+\.?\d*)', page_text)
            if beta_match:
                data['ë² íƒ€'] = beta_match.group(1)
        
        # ê¸°ì¡´ PER, PBR, ROE ë“± ê¸°ë³¸ ì§€í‘œ ì¶”ì¶œ
        patterns = {
            'PER': r'PER[^\d]*?([+-]?\d+\.?\d*)ë°°',
            'PBR': r'PBR[^\d]*?([+-]?\d+\.?\d*)ë°°', 
            'ROE': r'ROE[^\d]*?([+-]?\d+\.?\d*)%?',
            'ë°°ë‹¹ìˆ˜ìµë¥ ': r'ë°°ë‹¹ìˆ˜ìµë¥ [^\d]*?([+-]?\d+\.?\d*)%',
            '52ì£¼ìµœê³ ': r'52ì£¼ìµœê³ [^\d]*?([,\d]+)',
            '52ì£¼ìµœì €': r'52ì£¼ìµœì €[^\d]*?([,\d]+)',
            'ì™¸êµ­ì¸ë¹„ìœ¨': r'ì™¸êµ­ì¸[^\d]*?([+-]?\d+\.?\d*)%',
            'ê¸°ê´€ë¹„ìœ¨': r'ê¸°ê´€[^\d]*?([+-]?\d+\.?\d*)%'
        }
        
        for key, pattern in patterns.items():
            if not data[key]:  # ì•„ì§ ê°’ì´ ì—†ëŠ” ê²½ìš°ë§Œ
                match = re.search(pattern, page_text)
                if match:
                    data[key] = match.group(1).replace(',', '') if ',' in match.group(1) else match.group(1)
        
        # í…Œì´ë¸” ê¸°ë°˜ ìƒì„¸ ì¶”ì¶œ (ê¸°ì¡´ ë¡œì§ ìœ ì§€í•˜ë˜ ê°œì„ )
        for table in soup.find_all('table'):
            for row in table.find_all('tr'):
                cells = row.find_all(['td', 'th'])
                if len(cells) >= 2:
                    for i, cell in enumerate(cells[:-1]):
                        cell_text = cell.get_text(strip=True)
                        next_cell_text = cells[i+1].get_text(strip=True)
                        
                        # ì¬ë¬´ ì§€í‘œ ë§¤í•‘
                        field_mapping = {
                            'PER': ['PER', 'per'],
                            'PBR': ['PBR', 'pbr'],
                            'ROE': ['ROE', 'roe'],
                            'ì‹œê°€ì´ì•¡': ['ì‹œê°€ì´ì•¡'],
                            'ë§¤ì¶œì•¡': ['ë§¤ì¶œì•¡', 'ë§¤ì¶œ'],
                            'ì˜ì—…ì´ìµ': ['ì˜ì—…ì´ìµ'],
                            'ë‹¹ê¸°ìˆœì´ìµ': ['ë‹¹ê¸°ìˆœì´ìµ', 'ìˆœì´ìµ'],
                            'ë¶€ì±„ë¹„ìœ¨': ['ë¶€ì±„ë¹„ìœ¨'],
                            'ìœ ë³´ìœ¨': ['ìœ ë³´ìœ¨'],
                            'ë°°ë‹¹ìˆ˜ìµë¥ ': ['ë°°ë‹¹ìˆ˜ìµë¥ ', 'ë°°ë‹¹ë¥ '],
                            'ë°°ë‹¹ê¸ˆ': ['ë°°ë‹¹ê¸ˆ', 'í˜„ê¸ˆë°°ë‹¹'],
                            '52ì£¼ìµœê³ ': ['52ì£¼ìµœê³ ', '52ì£¼ ìµœê³ '],
                            '52ì£¼ìµœì €': ['52ì£¼ìµœì €', '52ì£¼ ìµœì €'],
                            'ê±°ë˜ëŒ€ê¸ˆ': ['ê±°ë˜ëŒ€ê¸ˆ'],
                            'ì™¸êµ­ì¸ë¹„ìœ¨': ['ì™¸êµ­ì¸', 'ì™¸êµ­ì¸ ë¹„ìœ¨'],
                            'ê¸°ê´€ë¹„ìœ¨': ['ê¸°ê´€', 'ê¸°ê´€ ë¹„ìœ¨'],
                            'ë² íƒ€': ['ë² íƒ€', 'Beta']
                        }
                        
                        for field, keywords in field_mapping.items():
                            if any(keyword in cell_text for keyword in keywords) and not data[field]:
                                # ìˆ«ì ì¶”ì¶œ
                                if field in ['ë§¤ì¶œì•¡', 'ì˜ì—…ì´ìµ', 'ë‹¹ê¸°ìˆœì´ìµ']:
                                    # ì–µì› ë‹¨ìœ„ ì²˜ë¦¬
                                    value_match = re.search(r'([,\d]+)ì–µ?ì›?', next_cell_text)
                                    if value_match:
                                        data[field] = value_match.group(1).replace(',', '')
                                elif field in ['52ì£¼ìµœê³ ', '52ì£¼ìµœì €', 'ê±°ë˜ëŒ€ê¸ˆ']:
                                    # ì¼ë°˜ ìˆ«ì (ì‰¼í‘œ ì œê±°)
                                    value_match = re.search(r'([,\d]+)', next_cell_text)
                                    if value_match:
                                        data[field] = value_match.group(1).replace(',', '')
                                elif field in ['ë¶€ì±„ë¹„ìœ¨', 'ìœ ë³´ìœ¨', 'ë°°ë‹¹ìˆ˜ìµë¥ ', 'ì™¸êµ­ì¸ë¹„ìœ¨', 'ê¸°ê´€ë¹„ìœ¨']:
                                    # í¼ì„¼íŠ¸ ê°’
                                    value_match = re.search(r'([+-]?\d+\.?\d*)%?', next_cell_text)
                                    if value_match:
                                        data[field] = value_match.group(1)
                                elif field in ['PER', 'PBR', 'ROE', 'ë² íƒ€']:
                                    # ë°°ìˆ˜ë‚˜ ë¹„ìœ¨
                                    value_match = re.search(r'([+-]?\d+\.?\d*)', next_cell_text)
                                    if value_match:
                                        data[field] = value_match.group(1)
                                elif field == 'ì‹œê°€ì´ì•¡':
                                    # ì‹œê°€ì´ì•¡ (ì–µì›)
                                    value_match = re.search(r'([,\d]+)ì–µ?ì›?', next_cell_text)
                                    if value_match:
                                        data[field] = value_match.group(1).replace(',', '')
                                elif field == 'ë°°ë‹¹ê¸ˆ':
                                    # ë°°ë‹¹ê¸ˆ (ì›)
                                    value_match = re.search(r'([,\d]+)ì›?', next_cell_text)
                                    if value_match:
                                        data[field] = value_match.group(1).replace(',', '')
        
        return tuple(data.values())
        
    except Exception as e:
        print(f"{name} ë°ì´í„° ìˆ˜ì§‘ ì˜¤ë¥˜: {str(e)}")
        return ('',) * 18  # 18ê°œ ë¹ˆ ê°’ ë°˜í™˜

def get_stock_data():
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}
    stock_data = []
    
    for market_type in [0, 1]:
        market_name = "ì½”ìŠ¤í”¼" if market_type == 0 else "ì½”ìŠ¤ë‹¥"
        print(f"\n{market_name} ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
        
        # ì „ì²´ í˜ì´ì§€ ìˆ˜ í™•ì¸
        url = f'https://finance.naver.com/sise/sise_market_sum.naver?sosok={market_type}&page=1'
        response = requests.get(url, headers=headers)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        max_page = 1
        page_nav = soup.select('td.pgRR > a')
        if page_nav:
            max_page = int(page_nav[0]['href'].split('=')[-1])
        
        print(f"{market_name} ì „ì²´ í˜ì´ì§€ ìˆ˜: {max_page}")
        
        # ì „ì²´ í˜ì´ì§€ ìˆ˜ì§‘
        for page in range(1, max_page + 1):
            print(f"í˜ì´ì§€ {page}/{max_page} ìˆ˜ì§‘ ì¤‘...")
            url = f'https://finance.naver.com/sise/sise_market_sum.naver?sosok={market_type}&page={page}'
            response = requests.get(url, headers=headers)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            rows = soup.select('table.type_2 tr')[1:]
            if not rows:
                break
            
            collected = 0
            for row in rows:
                cols = row.select('td')
                if len(cols) <= 1:
                    continue
                
                try:
                    name_element = cols[1].select_one('a')
                    if not name_element:
                        continue
                    
                    name = name_element.text.strip()
                    if not is_regular_stock(name):
                        continue
                    
                    code = name_element['href'].split('=')[-1]
                    current_price = cols[2].text.strip().replace(',', '')
                    current_volume = cols[9].text.strip().replace(',', '') if len(cols) > 9 else ''
                    
                    # ê°œë³„ ì¢…ëª© í˜ì´ì§€ì—ì„œ ìƒì„¸ ë°ì´í„° ìˆ˜ì§‘
                    time.sleep(0.3)  # ì„œë²„ ë¶€í•˜ ë°©ì§€
                    (per, pbr, roe, market_cap, sales, operating_profit, net_income, 
                     debt_ratio, retention_ratio, dividend_yield, dividend,
                     high_52w, low_52w, trading_value, foreign_ratio, 
                     institutional_ratio, beta, sector) = get_individual_stock_data(code, name)
                    
                    # ETF/í€ë“œ ì¶”ê°€ í•„í„°ë§: PER, PBR, ROEê°€ ëª¨ë‘ ë¹„ì–´ìˆìœ¼ë©´ ì œì™¸
                    if not per and not pbr and not roe:
                        print(f"  - {name}: PER/PBR/ROE ë°ì´í„° ì—†ìŒ (ETF/í€ë“œë¡œ ì¶”ì •) - ì œì™¸")
                        continue
                    
                    # ì¼ë³„ ì‹œì„¸ í˜ì´ì§€ì—ì„œ ì „ì¼ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
                    time.sleep(0.1)
                    daily_url = f'https://finance.naver.com/item/sise_day.naver?code={code}'
                    daily_response = requests.get(daily_url, headers=headers)
                    daily_soup = BeautifulSoup(daily_response.text, 'html.parser')
                    
                    daily_rows = daily_soup.select('table.type2 tr')
                    valid_rows = [r for r in daily_rows if r.select('td.num')]
                    
                    prev_close = ''
                    prev_volume = ''
                    if len(valid_rows) >= 2:
                        yesterday_row = valid_rows[1]
                        yesterday_cols = yesterday_row.select('td.num')
                        if len(yesterday_cols) >= 6:
                            prev_close = yesterday_cols[0].text.strip().replace(',', '')
                            prev_volume = yesterday_cols[5].text.strip().replace(',', '')
                    
                    # ê±°ë˜ëŸ‰ ì¦ê°ìœ¨ ê³„ì‚°
                    volume_change_rate = calculate_volume_change_rate(current_volume, prev_volume)
                    
                    if all([name, code, current_price]):
                        stock_data.append({
                            'ì¢…ëª©ëª…': name,
                            'ì¢…ëª©ì½”ë“œ': code,
                            'ì‹œì¥êµ¬ë¶„': market_name,
                            'ì—…ì¢…': sector,
                            'í˜„ì¬ê°€': current_price,
                            'ì „ì¼ì¢…ê°€': prev_close,
                            'ê±°ë˜ëŸ‰': current_volume,
                            'ì „ì¼ê±°ë˜ëŸ‰': prev_volume,
                            'ê±°ë˜ëŸ‰ì¦ê°ìœ¨': volume_change_rate,
                            'ê±°ë˜ëŒ€ê¸ˆ': trading_value,
                            'PER': per,
                            'PBR': pbr,
                            'ROE': roe,
                            'ì‹œê°€ì´ì•¡': market_cap,
                            'ë§¤ì¶œì•¡': sales,
                            'ì˜ì—…ì´ìµ': operating_profit,
                            'ë‹¹ê¸°ìˆœì´ìµ': net_income,
                            'ë¶€ì±„ë¹„ìœ¨': debt_ratio,
                            'ìœ ë³´ìœ¨': retention_ratio,
                            'ë°°ë‹¹ìˆ˜ìµë¥ ': dividend_yield,
                            'ë°°ë‹¹ê¸ˆ': dividend,
                            '52ì£¼ìµœê³ ': high_52w,
                            '52ì£¼ìµœì €': low_52w,
                            'ì™¸êµ­ì¸ë¹„ìœ¨': foreign_ratio,
                            'ê¸°ê´€ë¹„ìœ¨': institutional_ratio,
                            'ë² íƒ€': beta,
                            'ìˆ˜ì§‘ì¼ì': datetime.now().strftime('%Y-%m-%d')
                        })
                        collected += 1
                        
                        # ì§„í–‰ìƒí™© ì¶œë ¥
                        if collected % 10 == 0:
                            print(f"  - {collected}ê°œ ì¢…ëª© ìˆ˜ì§‘ ì™„ë£Œ (ìµœê·¼: {name} - ì—…ì¢…: {sector})")
                
                except Exception as e:
                    print(f"ì˜¤ë¥˜ ë°œìƒ - ì¢…ëª©: {name if 'name' in locals() else 'ì•Œ ìˆ˜ ì—†ìŒ'}, ì˜¤ë¥˜: {str(e)}")
                    continue
            
            print(f"í˜ì´ì§€ {page}ì—ì„œ {collected}ê°œ ì¢…ëª© ìˆ˜ì§‘ ì™„ë£Œ")
            
            # ì¤‘ê°„ ì €ì¥ (100í˜ì´ì§€ë§ˆë‹¤)
            if page % 100 == 0:
                temp_df = pd.DataFrame(stock_data)
                temp_filename = f'temp_stock_data_{datetime.now().strftime("%Y%m%d_%H%M")}_{page}.xlsx'
                temp_df.to_excel(temp_filename, index=False)
                print(f"ì¤‘ê°„ ì €ì¥: {temp_filename}")
    
    return stock_data

def main():
    print("=== ì „ì²´ ì¢…ëª© ìƒì„¸ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ ===")
    print("ìˆ˜ì§‘ ë°ì´í„°: 26ê°œ í•„ë“œ (ì¬ë¬´ì§€í‘œ, íˆ¬ììì •ë³´, ë°°ë‹¹ì •ë³´, ê±°ë˜ëŸ‰ì¦ê°ìœ¨ ë“±)")
    print("ì˜ˆìƒ ì†Œìš”ì‹œê°„: 3-4ì‹œê°„ (ì „ì²´ ì¢…ëª© ì•½ 2000-3000ê°œ)")
    print("ì£¼ì˜: ìˆ˜ì§‘ ì¤‘ ì¤‘ë‹¨í•˜ì§€ ë§ˆì„¸ìš”. 100í˜ì´ì§€ë§ˆë‹¤ ì¤‘ê°„ ì €ì¥ë©ë‹ˆë‹¤.\n")
    
    start_time = datetime.now()
    stock_data = get_stock_data()
    end_time = datetime.now()
    
    if stock_data:
        df = pd.DataFrame(stock_data)
        filename = f'full_stock_data_detailed_{datetime.now().strftime("%Y%m%d_%H%M")}.xlsx'
        df.to_excel(filename, index=False)
        
        print(f"\nğŸ‰ ìˆ˜ì§‘ ì™„ë£Œ!")
        print(f"ì´ {len(stock_data)}ê°œ ì¢…ëª©ì˜ ìƒì„¸ ë°ì´í„°ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"ì†Œìš”ì‹œê°„: {end_time - start_time}")
        
        # ë°ì´í„° í’ˆì§ˆ í™•ì¸
        print(f"\nğŸ“Š ë°ì´í„° í’ˆì§ˆ:")
        print(f"PER ë°ì´í„° ìˆëŠ” ì¢…ëª©: {df['PER'].notna().sum()}/{len(df)}")
        print(f"PBR ë°ì´í„° ìˆëŠ” ì¢…ëª©: {df['PBR'].notna().sum()}/{len(df)}")
        print(f"ROE ë°ì´í„° ìˆëŠ” ì¢…ëª©: {df['ROE'].notna().sum()}/{len(df)}")
        print(f"ì‹œê°€ì´ì•¡ ë°ì´í„° ìˆëŠ” ì¢…ëª©: {df['ì‹œê°€ì´ì•¡'].notna().sum()}/{len(df)}")
        print(f"ì—…ì¢… ë°ì´í„° ìˆëŠ” ì¢…ëª©: {df['ì—…ì¢…'].notna().sum()}/{len(df)}")
        print(f"ë§¤ì¶œì•¡ ë°ì´í„° ìˆëŠ” ì¢…ëª©: {df['ë§¤ì¶œì•¡'].notna().sum()}/{len(df)}")
        print(f"ë°°ë‹¹ìˆ˜ìµë¥  ë°ì´í„° ìˆëŠ” ì¢…ëª©: {df['ë°°ë‹¹ìˆ˜ìµë¥ '].notna().sum()}/{len(df)}")
        print(f"ê±°ë˜ëŸ‰ì¦ê°ìœ¨ ë°ì´í„° ìˆëŠ” ì¢…ëª©: {df['ê±°ë˜ëŸ‰ì¦ê°ìœ¨'].notna().sum()}/{len(df)}")
        
        # ì‹œê°€ì´ì•¡ë³„ ë¶„í¬ í™•ì¸
        df['ì‹œê°€ì´ì•¡'] = pd.to_numeric(df['ì‹œê°€ì´ì•¡'], errors='coerce')
        print(f"\nğŸ“ˆ ì‹œê°€ì´ì•¡ ë¶„í¬:")
        print(f"10ì¡° ì´ìƒ ëŒ€í˜•ì£¼: {(df['ì‹œê°€ì´ì•¡'] >= 100000).sum()}ê°œ")
        print(f"1ì¡°~10ì¡° ì¤‘í˜•ì£¼: {((df['ì‹œê°€ì´ì•¡'] >= 10000) & (df['ì‹œê°€ì´ì•¡'] < 100000)).sum()}ê°œ")
        print(f"1ì¡° ë¯¸ë§Œ ì†Œí˜•ì£¼: {(df['ì‹œê°€ì´ì•¡'] < 10000).sum()}ê°œ")
        
        # ê±°ë˜ëŸ‰ ì¦ê°ìœ¨ í†µê³„
        df['ê±°ë˜ëŸ‰ì¦ê°ìœ¨_num'] = pd.to_numeric(df['ê±°ë˜ëŸ‰ì¦ê°ìœ¨'], errors='coerce')
        volume_stats = df['ê±°ë˜ëŸ‰ì¦ê°ìœ¨_num'].describe()
        print(f"\nğŸ“Š ê±°ë˜ëŸ‰ ì¦ê°ìœ¨ í†µê³„:")
        print(f"í‰ê· : {volume_stats['mean']:.2f}%")
        print(f"ì¤‘ì•™ê°’: {volume_stats['50%']:.2f}%")
        print(f"ìµœëŒ€: {volume_stats['max']:.2f}%")
        print(f"ìµœì†Œ: {volume_stats['min']:.2f}%")
        
        # ì—…ì¢…ë³„ ë¶„í¬ í™•ì¸
        print(f"\nğŸ¢ ì—…ì¢…ë³„ ë¶„í¬ (ìƒìœ„ 10ê°œ):")
        sector_counts = df['ì—…ì¢…'].value_counts().head(10)
        for sector, count in sector_counts.items():
            print(f"{sector}: {count}ê°œ")
        
        # êµ¬ê¸€ ì‹œíŠ¸ ì—…ë¡œë“œ
        print(f"\nğŸ“¤ êµ¬ê¸€ ì‹œíŠ¸ ì—…ë¡œë“œ ì‹œì‘...")
        uploader = GoogleSheetsUploader()
        
        if uploader.gc:  # êµ¬ê¸€ ì‹œíŠ¸ ì—°ê²°ì´ ì„±ê³µí•œ ê²½ìš°
            # ê¸°ì¡´ Stock Analyzerì™€ ë™ì¼í•œ ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ì‚¬ìš©
            today = datetime.now().strftime('%Y-%m-%d')
            spreadsheet_name = f"ì£¼ì‹ë¶„ì„ê²°ê³¼_{today}"
            sheet_name = f"ğŸ’¾_ì „ì²´ì¢…ëª©ë°ì´í„°_{datetime.now().strftime('%H%M')}"
            
            success = uploader.upload_dataframe(df, spreadsheet_name, sheet_name)
            
            if success:
                # í—¤ë” í¬ë§· ì ìš©
                uploader.format_sheet_headers(spreadsheet_name, sheet_name)
                
                # ìŠ¤í”„ë ˆë“œì‹œíŠ¸ URL ì¶œë ¥
                url = uploader.get_spreadsheet_url(spreadsheet_name)
                if url:
                    print(f"ğŸ“Š êµ¬ê¸€ ì‹œíŠ¸ ë§í¬: {url}")
                    
                print(f"âœ… êµ¬ê¸€ ì‹œíŠ¸ ì—…ë¡œë“œ ì™„ë£Œ!")
                print(f"ğŸ“‹ ì‹œíŠ¸ëª…: {sheet_name}")
                print(f"ğŸ“ ê¸°ì¡´ ì—­ë°œìƒ íˆ¬ì ë°ì´í„°ì™€ ë™ì¼í•œ ìŠ¤í”„ë ˆë“œì‹œíŠ¸ì— ìƒˆ ì‹œíŠ¸ë¡œ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")
            else:
                print(f"âŒ êµ¬ê¸€ ì‹œíŠ¸ ì—…ë¡œë“œ ì‹¤íŒ¨")
        else:
            print(f"âš ï¸ êµ¬ê¸€ ì‹œíŠ¸ ì—°ê²° ì‹¤íŒ¨ë¡œ ì¸í•´ ì—…ë¡œë“œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            print(f"ğŸ’¾ ì—‘ì…€ íŒŒì¼ë¡œë§Œ ì €ì¥ë¨: {filename}")
            
    else:
        print("\nìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()